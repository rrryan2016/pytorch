{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 \n",
    "Environment: Jupyter notebook on Windows 10 with Python 3.7 and pytorch 1.5.0+cpu\n",
    "## Content 1: Basic Operation of Pytorch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build of Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5,3)\n",
    "x = torch.rand(5,3)\n",
    "x = torch.zeros(5,3,dtype=torch.long) # x = torch.zeros(5,3).long()\n",
    "x = torch.tensor([5.5,3])\n",
    "y = x.new_ones(5,3)\n",
    "y = torch.randn_like(x) # x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation of Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4,4)\n",
    "y = torch.rand(4,4)\n",
    "# for add\n",
    "x + y # torch.add(x,y)\n",
    "y.add_(x) # in-place加法\n",
    "# for index\n",
    "x[1:,1:]\n",
    "# for resize\n",
    "z = x.view(16)\n",
    "z = x.view(2,-1)\n",
    "# pick up the value\n",
    "single_x = torch.randn(1)\n",
    "single_x.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content 2: build a 3-layer NN with numpy \n",
    "include 2 hidden layers, use ReLU as activiation and L2 Loss without bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Loss: 13430251755.919409\n",
      "Iteration: 10 \t Loss: 867929329.6726524\n",
      "Iteration: 20 \t Loss: 365050665.71796906\n",
      "Iteration: 30 \t Loss: 184701444.36330676\n",
      "Iteration: 40 \t Loss: 104247990.03883037\n",
      "Iteration: 50 \t Loss: 63306683.100874305\n",
      "Iteration: 60 \t Loss: 40601806.934961945\n",
      "Iteration: 70 \t Loss: 27105124.950468913\n",
      "Iteration: 80 \t Loss: 18701197.68160733\n",
      "Iteration: 90 \t Loss: 13247770.075977009\n",
      "Iteration: 100 \t Loss: 9584281.895299733\n",
      "Iteration: 110 \t Loss: 7067768.972094469\n",
      "Iteration: 120 \t Loss: 5295195.473178003\n",
      "Iteration: 130 \t Loss: 4023041.9189234786\n",
      "Iteration: 140 \t Loss: 3094149.0525166057\n",
      "Iteration: 150 \t Loss: 2403189.5691689714\n",
      "Iteration: 160 \t Loss: 1883263.8470771837\n",
      "Iteration: 170 \t Loss: 1489201.9455094072\n",
      "Iteration: 180 \t Loss: 1186404.674904337\n",
      "Iteration: 190 \t Loss: 952002.9969203548\n",
      "Iteration: 200 \t Loss: 768375.9926026172\n",
      "Iteration: 210 \t Loss: 623302.1282526166\n",
      "Iteration: 220 \t Loss: 507980.71195484075\n",
      "Iteration: 230 \t Loss: 415685.61030439823\n",
      "Iteration: 240 \t Loss: 341595.3420744398\n",
      "Iteration: 250 \t Loss: 281753.87514057074\n",
      "Iteration: 260 \t Loss: 233142.4883051684\n",
      "Iteration: 270 \t Loss: 193493.7678139604\n",
      "Iteration: 280 \t Loss: 161033.52913896024\n",
      "Iteration: 290 \t Loss: 134368.17903301207\n",
      "Iteration: 300 \t Loss: 112376.34362550327\n",
      "Iteration: 310 \t Loss: 94188.92450763748\n",
      "Iteration: 320 \t Loss: 79163.07313608183\n",
      "Iteration: 330 \t Loss: 66741.35785082\n",
      "Iteration: 340 \t Loss: 56477.97149355116\n",
      "Iteration: 350 \t Loss: 47891.56917723213\n",
      "Iteration: 360 \t Loss: 40689.913811001345\n",
      "Iteration: 370 \t Loss: 34630.06957071826\n",
      "Iteration: 380 \t Loss: 29517.874346654757\n",
      "Iteration: 390 \t Loss: 25200.189654162874\n",
      "Iteration: 400 \t Loss: 21544.548947164734\n",
      "Iteration: 410 \t Loss: 18447.5729648424\n",
      "Iteration: 420 \t Loss: 15816.025300198273\n",
      "Iteration: 430 \t Loss: 13576.343122983077\n",
      "Iteration: 440 \t Loss: 11668.67032825729\n",
      "Iteration: 450 \t Loss: 10039.408292516637\n",
      "Iteration: 460 \t Loss: 8646.686710706195\n",
      "Iteration: 470 \t Loss: 7454.8814119017\n",
      "Iteration: 480 \t Loss: 6437.149965743523\n",
      "Iteration: 490 \t Loss: 5564.609210621\n",
      "Iteration: 500 \t Loss: 4814.543763127709\n",
      "Iteration: 510 \t Loss: 4169.080365274289\n",
      "Iteration: 520 \t Loss: 3613.0322287535505\n",
      "Iteration: 530 \t Loss: 3133.3371479460084\n",
      "Iteration: 540 \t Loss: 2719.4003385978795\n",
      "Iteration: 550 \t Loss: 2361.6912444488185\n",
      "Iteration: 560 \t Loss: 2052.39989077968\n",
      "Iteration: 570 \t Loss: 1784.6408336135084\n",
      "Iteration: 580 \t Loss: 1552.7669517850086\n",
      "Iteration: 590 \t Loss: 1351.7564530763766\n",
      "Iteration: 600 \t Loss: 1177.4573368850185\n",
      "Iteration: 610 \t Loss: 1026.1102321253286\n",
      "Iteration: 620 \t Loss: 894.6815730923254\n",
      "Iteration: 630 \t Loss: 780.4377506480552\n",
      "Iteration: 640 \t Loss: 681.0982958673943\n",
      "Iteration: 650 \t Loss: 594.6702635980369\n",
      "Iteration: 660 \t Loss: 519.4288136668283\n",
      "Iteration: 670 \t Loss: 453.88038592982036\n",
      "Iteration: 680 \t Loss: 396.75950327712087\n",
      "Iteration: 690 \t Loss: 346.96240228213753\n",
      "Iteration: 700 \t Loss: 303.51855019710746\n",
      "Iteration: 710 \t Loss: 265.61328465242394\n",
      "Iteration: 720 \t Loss: 232.51120853922023\n",
      "Iteration: 730 \t Loss: 203.60344464104244\n",
      "Iteration: 740 \t Loss: 178.3437915243636\n",
      "Iteration: 750 \t Loss: 156.2607327854121\n",
      "Iteration: 760 \t Loss: 136.95584010352184\n",
      "Iteration: 770 \t Loss: 120.06784172909474\n",
      "Iteration: 780 \t Loss: 105.28903282989887\n",
      "Iteration: 790 \t Loss: 92.35495223798897\n",
      "Iteration: 800 \t Loss: 81.02768577250102\n",
      "Iteration: 810 \t Loss: 71.10936591339646\n",
      "Iteration: 820 \t Loss: 62.41631191515096\n",
      "Iteration: 830 \t Loss: 54.79894431582405\n",
      "Iteration: 840 \t Loss: 48.12289853917033\n",
      "Iteration: 850 \t Loss: 42.267454218945566\n",
      "Iteration: 860 \t Loss: 37.13033402658386\n",
      "Iteration: 870 \t Loss: 32.62526736034346\n",
      "Iteration: 880 \t Loss: 28.67174178763945\n",
      "Iteration: 890 \t Loss: 25.202235789121502\n",
      "Iteration: 900 \t Loss: 22.156106511160164\n",
      "Iteration: 910 \t Loss: 19.48134169306292\n",
      "Iteration: 920 \t Loss: 17.1324652358524\n",
      "Iteration: 930 \t Loss: 15.068190955941745\n",
      "Iteration: 940 \t Loss: 13.255434386898518\n",
      "Iteration: 950 \t Loss: 11.662058003063686\n",
      "Iteration: 960 \t Loss: 10.261705421206113\n",
      "Iteration: 970 \t Loss: 9.031077159276673\n",
      "Iteration: 980 \t Loss: 7.9486627474948435\n",
      "Iteration: 990 \t Loss: 6.997035827992893\n",
      "Iteration: 1000 \t Loss: 6.159847041601188\n",
      "Iteration: 1010 \t Loss: 5.423555453251895\n",
      "Iteration: 1020 \t Loss: 4.775998894587781\n",
      "Iteration: 1030 \t Loss: 4.206069621773566\n",
      "Iteration: 1040 \t Loss: 3.7046963766427887\n",
      "Iteration: 1050 \t Loss: 3.263138870646228\n",
      "Iteration: 1060 \t Loss: 2.8746450037048534\n",
      "Iteration: 1070 \t Loss: 2.532702492290846\n",
      "Iteration: 1080 \t Loss: 2.2316028857170824\n",
      "Iteration: 1090 \t Loss: 1.9665372083648995\n",
      "Iteration: 1100 \t Loss: 1.7329450098590284\n",
      "Iteration: 1110 \t Loss: 1.5273419804189066\n",
      "Iteration: 1120 \t Loss: 1.3462461798703904\n",
      "Iteration: 1130 \t Loss: 1.1866936953633254\n",
      "Iteration: 1140 \t Loss: 1.046171149750461\n",
      "Iteration: 1150 \t Loss: 0.9222804233054764\n",
      "Iteration: 1160 \t Loss: 0.8131904300389634\n",
      "Iteration: 1170 \t Loss: 0.7170150979839948\n",
      "Iteration: 1180 \t Loss: 0.6322716074749261\n",
      "Iteration: 1190 \t Loss: 0.5575967666352413\n",
      "Iteration: 1200 \t Loss: 0.4917254264399788\n",
      "Iteration: 1210 \t Loss: 0.4337080792951905\n",
      "Iteration: 1220 \t Loss: 0.38253967081184936\n",
      "Iteration: 1230 \t Loss: 0.33742759213124895\n",
      "Iteration: 1240 \t Loss: 0.29765039852786224\n",
      "Iteration: 1250 \t Loss: 0.26257947967406237\n",
      "Iteration: 1260 \t Loss: 0.23165917863886665\n",
      "Iteration: 1270 \t Loss: 0.20438727418996994\n",
      "Iteration: 1280 \t Loss: 0.18032266353014217\n",
      "Iteration: 1290 \t Loss: 0.15911298530666104\n",
      "Iteration: 1300 \t Loss: 0.1404014521193158\n",
      "Iteration: 1310 \t Loss: 0.12389906851778461\n",
      "Iteration: 1320 \t Loss: 0.10933235526106803\n",
      "Iteration: 1330 \t Loss: 0.09648645865366161\n",
      "Iteration: 1340 \t Loss: 0.08515672398678699\n",
      "Iteration: 1350 \t Loss: 0.07515842772196657\n",
      "Iteration: 1360 \t Loss: 0.06633479562298014\n",
      "Iteration: 1370 \t Loss: 0.05854992160879775\n",
      "Iteration: 1380 \t Loss: 0.05168219472482799\n",
      "Iteration: 1390 \t Loss: 0.04562158838589086\n",
      "Iteration: 1400 \t Loss: 0.0402700823435695\n",
      "Iteration: 1410 \t Loss: 0.03555064296195725\n",
      "Iteration: 1420 \t Loss: 0.031384518326848604\n",
      "Iteration: 1430 \t Loss: 0.02770832983339065\n",
      "Iteration: 1440 \t Loss: 0.024461056824892313\n",
      "Iteration: 1450 \t Loss: 0.021596442423455396\n",
      "Iteration: 1460 \t Loss: 0.01906838754009507\n",
      "Iteration: 1470 \t Loss: 0.016836291536484284\n",
      "Iteration: 1480 \t Loss: 0.014865491746560766\n",
      "Iteration: 1490 \t Loss: 0.013126129725055134\n",
      "Iteration: 1500 \t Loss: 0.011590491249849112\n",
      "Iteration: 1510 \t Loss: 0.010235054634451908\n",
      "Iteration: 1520 \t Loss: 0.009037635054900243\n",
      "Iteration: 1530 \t Loss: 0.007981223208680385\n",
      "Iteration: 1540 \t Loss: 0.0070482008079955135\n",
      "Iteration: 1550 \t Loss: 0.006224112992808379\n",
      "Iteration: 1560 \t Loss: 0.0054968677234091025\n",
      "Iteration: 1570 \t Loss: 0.004854586088293323\n",
      "Iteration: 1580 \t Loss: 0.004287622768220985\n",
      "Iteration: 1590 \t Loss: 0.003786535777220282\n",
      "Iteration: 1600 \t Loss: 0.003344437868711604\n",
      "Iteration: 1610 \t Loss: 0.002953885653971551\n",
      "Iteration: 1620 \t Loss: 0.002608887089248473\n",
      "Iteration: 1630 \t Loss: 0.0023043737297214154\n",
      "Iteration: 1640 \t Loss: 0.0020354047555264697\n",
      "Iteration: 1650 \t Loss: 0.001797917472097241\n",
      "Iteration: 1660 \t Loss: 0.001588023936947321\n",
      "Iteration: 1670 \t Loss: 0.0014027384679559083\n",
      "Iteration: 1680 \t Loss: 0.0012391226896442148\n",
      "Iteration: 1690 \t Loss: 0.001094519934123771\n",
      "Iteration: 1700 \t Loss: 0.0009668846805229715\n",
      "Iteration: 1710 \t Loss: 0.0008541235047328189\n",
      "Iteration: 1720 \t Loss: 0.0007545434152654497\n",
      "Iteration: 1730 \t Loss: 0.0006665211868199228\n",
      "Iteration: 1740 \t Loss: 0.0005888103493876851\n",
      "Iteration: 1750 \t Loss: 0.0005201879887693331\n",
      "Iteration: 1760 \t Loss: 0.00045952239667152453\n",
      "Iteration: 1770 \t Loss: 0.00040597613533418375\n",
      "Iteration: 1780 \t Loss: 0.0003586604828391533\n",
      "Iteration: 1790 \t Loss: 0.0003168780600613601\n",
      "Iteration: 1800 \t Loss: 0.0002799359500989214\n",
      "Iteration: 1810 \t Loss: 0.00024731826260483225\n",
      "Iteration: 1820 \t Loss: 0.0002185117437360145\n",
      "Iteration: 1830 \t Loss: 0.00019304374054756365\n",
      "Iteration: 1840 \t Loss: 0.00017056296924255397\n",
      "Iteration: 1850 \t Loss: 0.00015069509535093763\n",
      "Iteration: 1860 \t Loss: 0.00013313659591750976\n",
      "Iteration: 1870 \t Loss: 0.00011763456991788421\n",
      "Iteration: 1880 \t Loss: 0.00010393509931435523\n",
      "Iteration: 1890 \t Loss: 9.183583504694297e-05\n",
      "Iteration: 1900 \t Loss: 8.113738583473645e-05\n",
      "Iteration: 1910 \t Loss: 7.169314759934646e-05\n",
      "Iteration: 1920 \t Loss: 6.334603830119589e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1930 \t Loss: 5.596794270747995e-05\n",
      "Iteration: 1940 \t Loss: 4.945441649661509e-05\n",
      "Iteration: 1950 \t Loss: 4.3697509487600185e-05\n",
      "Iteration: 1960 \t Loss: 3.86104502485074e-05\n",
      "Iteration: 1970 \t Loss: 3.411649366236715e-05\n",
      "Iteration: 1980 \t Loss: 3.0146804973558384e-05\n",
      "Iteration: 1990 \t Loss: 2.6637923547790045e-05\n",
      "Iteration: 2000 \t Loss: 2.3536792079734416e-05\n",
      "Iteration: 2010 \t Loss: 2.079862985402101e-05\n",
      "Iteration: 2020 \t Loss: 1.837852032538316e-05\n",
      "Iteration: 2030 \t Loss: 1.62395457062219e-05\n",
      "Iteration: 2040 \t Loss: 1.4350053823940415e-05\n",
      "Iteration: 2050 \t Loss: 1.2680886699646669e-05\n",
      "Iteration: 2060 \t Loss: 1.1204862547716597e-05\n",
      "Iteration: 2070 \t Loss: 9.901330568742168e-06\n",
      "Iteration: 2080 \t Loss: 8.749741150540092e-06\n",
      "Iteration: 2090 \t Loss: 7.73146378670021e-06\n",
      "Iteration: 2100 \t Loss: 6.83235791638556e-06\n",
      "Iteration: 2110 \t Loss: 6.037593547085151e-06\n",
      "Iteration: 2120 \t Loss: 5.3354172462164125e-06\n",
      "Iteration: 2130 \t Loss: 4.714657522396746e-06\n",
      "Iteration: 2140 \t Loss: 4.166297636559192e-06\n",
      "Iteration: 2150 \t Loss: 3.681895506804662e-06\n",
      "Iteration: 2160 \t Loss: 3.2534925486408926e-06\n",
      "Iteration: 2170 \t Loss: 2.8752252983511355e-06\n",
      "Iteration: 2180 \t Loss: 2.5408662524026766e-06\n",
      "Iteration: 2190 \t Loss: 2.2453348740682785e-06\n",
      "Iteration: 2200 \t Loss: 1.9842286823870734e-06\n",
      "Iteration: 2210 \t Loss: 1.7535705890364643e-06\n",
      "Iteration: 2220 \t Loss: 1.549561212847057e-06\n",
      "Iteration: 2230 \t Loss: 1.369387054126752e-06\n",
      "Iteration: 2240 \t Loss: 1.2102108003494653e-06\n",
      "Iteration: 2250 \t Loss: 1.0694324325962612e-06\n",
      "Iteration: 2260 \t Loss: 9.45127889930096e-07\n",
      "Iteration: 2270 \t Loss: 8.352441545088831e-07\n",
      "Iteration: 2280 \t Loss: 7.381383208977064e-07\n",
      "Iteration: 2290 \t Loss: 6.523053362684572e-07\n",
      "Iteration: 2300 \t Loss: 5.764715216039219e-07\n",
      "Iteration: 2310 \t Loss: 5.094717784236188e-07\n",
      "Iteration: 2320 \t Loss: 4.5021817983874285e-07\n",
      "Iteration: 2330 \t Loss: 3.9789683266087536e-07\n",
      "Iteration: 2340 \t Loss: 3.5164410483381585e-07\n",
      "Iteration: 2350 \t Loss: 3.1076055698212275e-07\n",
      "Iteration: 2360 \t Loss: 2.7463752881008813e-07\n",
      "Iteration: 2370 \t Loss: 2.427216841758386e-07\n",
      "Iteration: 2380 \t Loss: 2.1449608451233177e-07\n",
      "Iteration: 2390 \t Loss: 1.8956449560400732e-07\n",
      "Iteration: 2400 \t Loss: 1.675368621447544e-07\n",
      "Iteration: 2410 \t Loss: 1.480545749660519e-07\n",
      "Iteration: 2420 \t Loss: 1.3085133903355365e-07\n",
      "Iteration: 2430 \t Loss: 1.1564309365729467e-07\n",
      "Iteration: 2440 \t Loss: 1.0219989888960101e-07\n",
      "Iteration: 2450 \t Loss: 9.03218125175804e-08\n",
      "Iteration: 2460 \t Loss: 7.982579165013697e-08\n",
      "Iteration: 2470 \t Loss: 7.054567352423246e-08\n",
      "Iteration: 2480 \t Loss: 6.234723396217947e-08\n",
      "Iteration: 2490 \t Loss: 5.510367862584318e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N,D_in,H_1,H_2,D_out = 64,1000,500,100,10  # N: batch size \n",
    "x = np.random.randn(N,D_in) # random initialize some data\n",
    "y = np.random.randn(N,D_out) # random initialize the label of the data\n",
    "w1 = np.random.randn(D_in,H_1) # 初始化权重\n",
    "w2 = np.random.randn(H_1,H_2)\n",
    "w3 = np.random.randn(H_2,D_out)\n",
    "learning_rate = 1e-9\n",
    "for i in range (2500):\n",
    "    # forward pass \n",
    "    h_1 = x.dot(w1)\n",
    "    h_relu_1 = np.maximum(h_1,0)\n",
    "    h_2 = h_relu_1.dot(w2)\n",
    "    h_relu_2 = np.maximum(h_2,0)\n",
    "    y_pred = h_relu_2.dot(w3)\n",
    "    # compute loss \n",
    "    loss = np.square(y_pred-y).sum()\n",
    "    if i%10==0:\n",
    "        print('Iteration:',i,'\\t Loss:',loss)\n",
    "    # BP - compute the gradient \n",
    "    grad_y_pred = 2.0 * (y_pred-y)\n",
    "    grad_w3 = h_relu_2.T.dot(grad_y_pred)\n",
    "    grad_h_relu_2 = grad_y_pred.dot(w3.T)\n",
    "    grad_h_2 = grad_h_relu_2.copy()\n",
    "    grad_h_2[h_2<0]=0\n",
    "    grad_w2 = h_relu_1.T.dot(grad_h_2)\n",
    "    grad_h_relu_1 = grad_h_2.dot(w2.T)\n",
    "    grad_h_1 = grad_h_relu_1.copy()\n",
    "    grad_h_1[h_1<0] = 0 \n",
    "    grad_w1 = x.T.dot(grad_h_1)\n",
    "       \n",
    "    # Update weights of w1 / w2 /w3 \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2 \n",
    "    w3 -= learning_rate * grad_w3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Content 3: from numpy to tensor \n",
    " Substitute the 3-layer NN code in content 2 to a tensor version \n",
    " ### 3-1 Substitude Numpy to Tensor directly \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Loss: 12128309248.0\n",
      "Iteration: 10 \t Loss: 992579712.0\n",
      "Iteration: 20 \t Loss: 415525760.0\n",
      "Iteration: 30 \t Loss: 212178896.0\n",
      "Iteration: 40 \t Loss: 120953536.0\n",
      "Iteration: 50 \t Loss: 74001664.0\n",
      "Iteration: 60 \t Loss: 47522568.0\n",
      "Iteration: 70 \t Loss: 31609106.0\n",
      "Iteration: 80 \t Loss: 21578164.0\n",
      "Iteration: 90 \t Loss: 15070702.0\n",
      "Iteration: 100 \t Loss: 10717125.0\n",
      "Iteration: 110 \t Loss: 7736689.0\n",
      "Iteration: 120 \t Loss: 5658017.5\n",
      "Iteration: 130 \t Loss: 4188123.75\n",
      "Iteration: 140 \t Loss: 3132390.0\n",
      "Iteration: 150 \t Loss: 2363919.5\n",
      "Iteration: 160 \t Loss: 1800603.0\n",
      "Iteration: 170 \t Loss: 1381721.625\n",
      "Iteration: 180 \t Loss: 1067721.625\n",
      "Iteration: 190 \t Loss: 830261.625\n",
      "Iteration: 200 \t Loss: 649233.75\n",
      "Iteration: 210 \t Loss: 510359.3125\n",
      "Iteration: 220 \t Loss: 403193.125\n",
      "Iteration: 230 \t Loss: 320027.6875\n",
      "Iteration: 240 \t Loss: 255065.0\n",
      "Iteration: 250 \t Loss: 204147.6875\n",
      "Iteration: 260 \t Loss: 164049.53125\n",
      "Iteration: 270 \t Loss: 132280.1875\n",
      "Iteration: 280 \t Loss: 107014.8984375\n",
      "Iteration: 290 \t Loss: 86834.90625\n",
      "Iteration: 300 \t Loss: 70663.703125\n",
      "Iteration: 310 \t Loss: 57658.5078125\n",
      "Iteration: 320 \t Loss: 47170.97265625\n",
      "Iteration: 330 \t Loss: 38685.9296875\n",
      "Iteration: 340 \t Loss: 31798.91796875\n",
      "Iteration: 350 \t Loss: 26196.15625\n",
      "Iteration: 360 \t Loss: 21624.36328125\n",
      "Iteration: 370 \t Loss: 17887.2109375\n",
      "Iteration: 380 \t Loss: 14824.3857421875\n",
      "Iteration: 390 \t Loss: 12308.0107421875\n",
      "Iteration: 400 \t Loss: 10237.271484375\n",
      "Iteration: 410 \t Loss: 8528.9599609375\n",
      "Iteration: 420 \t Loss: 7117.1015625\n",
      "Iteration: 430 \t Loss: 5948.5302734375\n",
      "Iteration: 440 \t Loss: 4979.24658203125\n",
      "Iteration: 450 \t Loss: 4173.6259765625\n",
      "Iteration: 460 \t Loss: 3503.6630859375\n",
      "Iteration: 470 \t Loss: 2945.06396484375\n",
      "Iteration: 480 \t Loss: 2478.7744140625\n",
      "Iteration: 490 \t Loss: 2088.948974609375\n",
      "Iteration: 500 \t Loss: 1762.752197265625\n",
      "Iteration: 510 \t Loss: 1489.35205078125\n",
      "Iteration: 520 \t Loss: 1259.716064453125\n",
      "Iteration: 530 \t Loss: 1066.751220703125\n",
      "Iteration: 540 \t Loss: 904.39453125\n",
      "Iteration: 550 \t Loss: 767.58203125\n",
      "Iteration: 560 \t Loss: 652.3336791992188\n",
      "Iteration: 570 \t Loss: 554.9617309570312\n",
      "Iteration: 580 \t Loss: 472.6689758300781\n",
      "Iteration: 590 \t Loss: 403.10211181640625\n",
      "Iteration: 600 \t Loss: 344.1435241699219\n",
      "Iteration: 610 \t Loss: 294.2234802246094\n",
      "Iteration: 620 \t Loss: 251.90061950683594\n",
      "Iteration: 630 \t Loss: 215.97372436523438\n",
      "Iteration: 640 \t Loss: 185.41156005859375\n",
      "Iteration: 650 \t Loss: 159.44635009765625\n",
      "Iteration: 660 \t Loss: 137.3134765625\n",
      "Iteration: 670 \t Loss: 118.50738525390625\n",
      "Iteration: 680 \t Loss: 102.43199157714844\n",
      "Iteration: 690 \t Loss: 88.72724914550781\n",
      "Iteration: 700 \t Loss: 77.01483154296875\n",
      "Iteration: 710 \t Loss: 66.98783111572266\n",
      "Iteration: 720 \t Loss: 58.385108947753906\n",
      "Iteration: 730 \t Loss: 51.00605010986328\n",
      "Iteration: 740 \t Loss: 44.6524772644043\n",
      "Iteration: 750 \t Loss: 39.177894592285156\n",
      "Iteration: 760 \t Loss: 34.4586181640625\n",
      "Iteration: 770 \t Loss: 30.38652992248535\n",
      "Iteration: 780 \t Loss: 26.85566520690918\n",
      "Iteration: 790 \t Loss: 23.78414535522461\n",
      "Iteration: 800 \t Loss: 21.117734909057617\n",
      "Iteration: 810 \t Loss: 18.792377471923828\n",
      "Iteration: 820 \t Loss: 16.761672973632812\n",
      "Iteration: 830 \t Loss: 14.974228858947754\n",
      "Iteration: 840 \t Loss: 13.418169975280762\n",
      "Iteration: 850 \t Loss: 12.042826652526855\n",
      "Iteration: 860 \t Loss: 10.832883834838867\n",
      "Iteration: 870 \t Loss: 9.763381004333496\n",
      "Iteration: 880 \t Loss: 8.82059097290039\n",
      "Iteration: 890 \t Loss: 7.98436164855957\n",
      "Iteration: 900 \t Loss: 7.241002082824707\n",
      "Iteration: 910 \t Loss: 6.5815582275390625\n",
      "Iteration: 920 \t Loss: 5.993346214294434\n",
      "Iteration: 930 \t Loss: 5.4652299880981445\n",
      "Iteration: 940 \t Loss: 4.994294166564941\n",
      "Iteration: 950 \t Loss: 4.575148582458496\n",
      "Iteration: 960 \t Loss: 4.194053649902344\n",
      "Iteration: 970 \t Loss: 3.8556575775146484\n",
      "Iteration: 980 \t Loss: 3.5525970458984375\n",
      "Iteration: 990 \t Loss: 3.2796380519866943\n",
      "Iteration: 1000 \t Loss: 3.0325260162353516\n",
      "Iteration: 1010 \t Loss: 2.8067240715026855\n",
      "Iteration: 1020 \t Loss: 2.604600191116333\n",
      "Iteration: 1030 \t Loss: 2.416666030883789\n",
      "Iteration: 1040 \t Loss: 2.2469115257263184\n",
      "Iteration: 1050 \t Loss: 2.093425750732422\n",
      "Iteration: 1060 \t Loss: 1.951985478401184\n",
      "Iteration: 1070 \t Loss: 1.8247437477111816\n",
      "Iteration: 1080 \t Loss: 1.7073185443878174\n",
      "Iteration: 1090 \t Loss: 1.5990581512451172\n",
      "Iteration: 1100 \t Loss: 1.4989275932312012\n",
      "Iteration: 1110 \t Loss: 1.4085785150527954\n",
      "Iteration: 1120 \t Loss: 1.3239526748657227\n",
      "Iteration: 1130 \t Loss: 1.2471773624420166\n",
      "Iteration: 1140 \t Loss: 1.1741925477981567\n",
      "Iteration: 1150 \t Loss: 1.1083992719650269\n",
      "Iteration: 1160 \t Loss: 1.046083688735962\n",
      "Iteration: 1170 \t Loss: 0.989290177822113\n",
      "Iteration: 1180 \t Loss: 0.9372225999832153\n",
      "Iteration: 1190 \t Loss: 0.8875259160995483\n",
      "Iteration: 1200 \t Loss: 0.8423982262611389\n",
      "Iteration: 1210 \t Loss: 0.799473226070404\n",
      "Iteration: 1220 \t Loss: 0.7601335644721985\n",
      "Iteration: 1230 \t Loss: 0.7226179838180542\n",
      "Iteration: 1240 \t Loss: 0.6878955960273743\n",
      "Iteration: 1250 \t Loss: 0.6553229093551636\n",
      "Iteration: 1260 \t Loss: 0.6248248815536499\n",
      "Iteration: 1270 \t Loss: 0.5963406562805176\n",
      "Iteration: 1280 \t Loss: 0.5702795386314392\n",
      "Iteration: 1290 \t Loss: 0.5455105304718018\n",
      "Iteration: 1300 \t Loss: 0.522199809551239\n",
      "Iteration: 1310 \t Loss: 0.5002423524856567\n",
      "Iteration: 1320 \t Loss: 0.47949928045272827\n",
      "Iteration: 1330 \t Loss: 0.4597056806087494\n",
      "Iteration: 1340 \t Loss: 0.4418181777000427\n",
      "Iteration: 1350 \t Loss: 0.42389434576034546\n",
      "Iteration: 1360 \t Loss: 0.4074530601501465\n",
      "Iteration: 1370 \t Loss: 0.39243176579475403\n",
      "Iteration: 1380 \t Loss: 0.37770912051200867\n",
      "Iteration: 1390 \t Loss: 0.3642313778400421\n",
      "Iteration: 1400 \t Loss: 0.35105380415916443\n",
      "Iteration: 1410 \t Loss: 0.33826664090156555\n",
      "Iteration: 1420 \t Loss: 0.3266299068927765\n",
      "Iteration: 1430 \t Loss: 0.31529149413108826\n",
      "Iteration: 1440 \t Loss: 0.304749071598053\n",
      "Iteration: 1450 \t Loss: 0.29459047317504883\n",
      "Iteration: 1460 \t Loss: 0.28537607192993164\n",
      "Iteration: 1470 \t Loss: 0.275890588760376\n",
      "Iteration: 1480 \t Loss: 0.26690244674682617\n",
      "Iteration: 1490 \t Loss: 0.25893402099609375\n",
      "Iteration: 1500 \t Loss: 0.2510210871696472\n",
      "Iteration: 1510 \t Loss: 0.24332742393016815\n",
      "Iteration: 1520 \t Loss: 0.2355426400899887\n",
      "Iteration: 1530 \t Loss: 0.22909609973430634\n",
      "Iteration: 1540 \t Loss: 0.22234366834163666\n",
      "Iteration: 1550 \t Loss: 0.21571624279022217\n",
      "Iteration: 1560 \t Loss: 0.2095818668603897\n",
      "Iteration: 1570 \t Loss: 0.20367132127285004\n",
      "Iteration: 1580 \t Loss: 0.19795960187911987\n",
      "Iteration: 1590 \t Loss: 0.1923999786376953\n",
      "Iteration: 1600 \t Loss: 0.1871291548013687\n",
      "Iteration: 1610 \t Loss: 0.1820622682571411\n",
      "Iteration: 1620 \t Loss: 0.1774001270532608\n",
      "Iteration: 1630 \t Loss: 0.17258214950561523\n",
      "Iteration: 1640 \t Loss: 0.16821061074733734\n",
      "Iteration: 1650 \t Loss: 0.16383104026317596\n",
      "Iteration: 1660 \t Loss: 0.15977743268013\n",
      "Iteration: 1670 \t Loss: 0.1556437760591507\n",
      "Iteration: 1680 \t Loss: 0.15210938453674316\n",
      "Iteration: 1690 \t Loss: 0.148295059800148\n",
      "Iteration: 1700 \t Loss: 0.14473268389701843\n",
      "Iteration: 1710 \t Loss: 0.14133067429065704\n",
      "Iteration: 1720 \t Loss: 0.13813066482543945\n",
      "Iteration: 1730 \t Loss: 0.1350940614938736\n",
      "Iteration: 1740 \t Loss: 0.13193000853061676\n",
      "Iteration: 1750 \t Loss: 0.1291790008544922\n",
      "Iteration: 1760 \t Loss: 0.1262703835964203\n",
      "Iteration: 1770 \t Loss: 0.1235399842262268\n",
      "Iteration: 1780 \t Loss: 0.12073269486427307\n",
      "Iteration: 1790 \t Loss: 0.11837847530841827\n",
      "Iteration: 1800 \t Loss: 0.11596377193927765\n",
      "Iteration: 1810 \t Loss: 0.1137930154800415\n",
      "Iteration: 1820 \t Loss: 0.11133080720901489\n",
      "Iteration: 1830 \t Loss: 0.10922518372535706\n",
      "Iteration: 1840 \t Loss: 0.10699310898780823\n",
      "Iteration: 1850 \t Loss: 0.10482660681009293\n",
      "Iteration: 1860 \t Loss: 0.10316036641597748\n",
      "Iteration: 1870 \t Loss: 0.10130423307418823\n",
      "Iteration: 1880 \t Loss: 0.09933285415172577\n",
      "Iteration: 1890 \t Loss: 0.09764786809682846\n",
      "Iteration: 1900 \t Loss: 0.096007339656353\n",
      "Iteration: 1910 \t Loss: 0.0941881313920021\n",
      "Iteration: 1920 \t Loss: 0.09246627241373062\n",
      "Iteration: 1930 \t Loss: 0.09083434194326401\n",
      "Iteration: 1940 \t Loss: 0.089403435587883\n",
      "Iteration: 1950 \t Loss: 0.08792941272258759\n",
      "Iteration: 1960 \t Loss: 0.08644847571849823\n",
      "Iteration: 1970 \t Loss: 0.08492112904787064\n",
      "Iteration: 1980 \t Loss: 0.08354229480028152\n",
      "Iteration: 1990 \t Loss: 0.08205575495958328\n",
      "Iteration: 2000 \t Loss: 0.08090758323669434\n",
      "Iteration: 2010 \t Loss: 0.07946164906024933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2020 \t Loss: 0.07824195176362991\n",
      "Iteration: 2030 \t Loss: 0.07737016677856445\n",
      "Iteration: 2040 \t Loss: 0.07608509063720703\n",
      "Iteration: 2050 \t Loss: 0.07489378750324249\n",
      "Iteration: 2060 \t Loss: 0.07390717417001724\n",
      "Iteration: 2070 \t Loss: 0.07254797965288162\n",
      "Iteration: 2080 \t Loss: 0.07158666104078293\n",
      "Iteration: 2090 \t Loss: 0.0705668181180954\n",
      "Iteration: 2100 \t Loss: 0.06951762735843658\n",
      "Iteration: 2110 \t Loss: 0.06847832351922989\n",
      "Iteration: 2120 \t Loss: 0.06766151636838913\n",
      "Iteration: 2130 \t Loss: 0.06673801690340042\n",
      "Iteration: 2140 \t Loss: 0.06566460430622101\n",
      "Iteration: 2150 \t Loss: 0.06492065638303757\n",
      "Iteration: 2160 \t Loss: 0.0641922876238823\n",
      "Iteration: 2170 \t Loss: 0.06301117688417435\n",
      "Iteration: 2180 \t Loss: 0.062305450439453125\n",
      "Iteration: 2190 \t Loss: 0.061484042555093765\n",
      "Iteration: 2200 \t Loss: 0.06063823774456978\n",
      "Iteration: 2210 \t Loss: 0.05968358740210533\n",
      "Iteration: 2220 \t Loss: 0.058904774487018585\n",
      "Iteration: 2230 \t Loss: 0.058205872774124146\n",
      "Iteration: 2240 \t Loss: 0.05745432525873184\n",
      "Iteration: 2250 \t Loss: 0.056856635957956314\n",
      "Iteration: 2260 \t Loss: 0.056189436465501785\n",
      "Iteration: 2270 \t Loss: 0.055442340672016144\n",
      "Iteration: 2280 \t Loss: 0.05494621768593788\n",
      "Iteration: 2290 \t Loss: 0.054135650396347046\n",
      "Iteration: 2300 \t Loss: 0.05346592515707016\n",
      "Iteration: 2310 \t Loss: 0.052951913326978683\n",
      "Iteration: 2320 \t Loss: 0.0523233637213707\n",
      "Iteration: 2330 \t Loss: 0.05171242728829384\n",
      "Iteration: 2340 \t Loss: 0.051107823848724365\n",
      "Iteration: 2350 \t Loss: 0.05046059936285019\n",
      "Iteration: 2360 \t Loss: 0.049926191568374634\n",
      "Iteration: 2370 \t Loss: 0.04938310384750366\n",
      "Iteration: 2380 \t Loss: 0.04890767112374306\n",
      "Iteration: 2390 \t Loss: 0.048287659883499146\n",
      "Iteration: 2400 \t Loss: 0.04775560647249222\n",
      "Iteration: 2410 \t Loss: 0.047249890863895416\n",
      "Iteration: 2420 \t Loss: 0.046680353581905365\n",
      "Iteration: 2430 \t Loss: 0.04620789736509323\n",
      "Iteration: 2440 \t Loss: 0.04573974013328552\n",
      "Iteration: 2450 \t Loss: 0.0453704409301281\n",
      "Iteration: 2460 \t Loss: 0.04484237730503082\n",
      "Iteration: 2470 \t Loss: 0.04449664428830147\n",
      "Iteration: 2480 \t Loss: 0.04411116987466812\n",
      "Iteration: 2490 \t Loss: 0.04356090724468231\n"
     ]
    }
   ],
   "source": [
    "N,D_in,H_1,H_2,D_out = 64,1000,500,100,10  # N: batch size \n",
    "x = torch.randn(N,D_in) # random initialize some data\n",
    "y = torch.randn(N,D_out) # random initialize the label of the data\n",
    "w1 = torch.randn(D_in,H_1) # 初始化权重\n",
    "w2 = torch.randn(H_1,H_2)\n",
    "w3 = torch.randn(H_2,D_out)\n",
    "learning_rate = 1e-9\n",
    "for i in range (2500):\n",
    "    # forward pass \n",
    "    h_1 = x.mm(w1)\n",
    "    h_relu_1 = h_1.clamp(min=0)\n",
    "    h_2 = h_relu_1.mm(w2)\n",
    "    h_relu_2 = h_2.clamp(min=0)\n",
    "    y_pred = h_relu_2.mm(w3)\n",
    "    # compute loss \n",
    "#     loss = np.square(y_pred-y).sum()\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if i%10==0:\n",
    "        print('Iteration:',i,'\\t Loss:',loss.item())\n",
    "    # BP - compute the gradient \n",
    "    grad_y_pred = 2.0 * (y_pred-y)\n",
    "    grad_w3 = h_relu_2.t().mm(grad_y_pred)\n",
    "    grad_h_relu_2 = grad_y_pred.mm(w3.t())\n",
    "    grad_h_2 = grad_h_relu_2.clone()\n",
    "    grad_h_2[h_2<0]=0\n",
    "    grad_w2 = h_relu_1.t().mm(grad_h_2)\n",
    "    grad_h_relu_1 = grad_h_2.mm(w2.t()) \n",
    "    grad_h_1 = grad_h_relu_1.clone()\n",
    "    grad_h_1[h_1<0] = 0 \n",
    "    grad_w1 = x.t().mm(grad_h_1)\n",
    "       \n",
    "    # Update weights of w1 / w2 /w3 \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2 \n",
    "    w3 -= learning_rate * grad_w3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 Use the autograd in Tensor to calculate gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Loss: 7237479424.0\n",
      "Iteration: 10 \t Loss: 949425856.0\n",
      "Iteration: 20 \t Loss: 406458464.0\n",
      "Iteration: 30 \t Loss: 211722880.0\n",
      "Iteration: 40 \t Loss: 123604576.0\n",
      "Iteration: 50 \t Loss: 77905000.0\n",
      "Iteration: 60 \t Loss: 51608620.0\n",
      "Iteration: 70 \t Loss: 35578484.0\n",
      "Iteration: 80 \t Loss: 25276276.0\n",
      "Iteration: 90 \t Loss: 18399188.0\n",
      "Iteration: 100 \t Loss: 13654684.0\n",
      "Iteration: 110 \t Loss: 10297267.0\n",
      "Iteration: 120 \t Loss: 7872497.0\n",
      "Iteration: 130 \t Loss: 6086071.0\n",
      "Iteration: 140 \t Loss: 4750702.0\n",
      "Iteration: 150 \t Loss: 3740612.0\n",
      "Iteration: 160 \t Loss: 2970578.5\n",
      "Iteration: 170 \t Loss: 2378975.0\n",
      "Iteration: 180 \t Loss: 1917891.875\n",
      "Iteration: 190 \t Loss: 1554897.875\n",
      "Iteration: 200 \t Loss: 1266561.75\n",
      "Iteration: 210 \t Loss: 1036079.1875\n",
      "Iteration: 220 \t Loss: 851239.125\n",
      "Iteration: 230 \t Loss: 701849.875\n",
      "Iteration: 240 \t Loss: 580485.5625\n",
      "Iteration: 250 \t Loss: 481454.21875\n",
      "Iteration: 260 \t Loss: 400345.5\n",
      "Iteration: 270 \t Loss: 333663.03125\n",
      "Iteration: 280 \t Loss: 278646.40625\n",
      "Iteration: 290 \t Loss: 233140.296875\n",
      "Iteration: 300 \t Loss: 195406.90625\n",
      "Iteration: 310 \t Loss: 164040.34375\n",
      "Iteration: 320 \t Loss: 137913.90625\n",
      "Iteration: 330 \t Loss: 116103.25\n",
      "Iteration: 340 \t Loss: 97866.5859375\n",
      "Iteration: 350 \t Loss: 82590.34375\n",
      "Iteration: 360 \t Loss: 69772.3125\n",
      "Iteration: 370 \t Loss: 59007.6171875\n",
      "Iteration: 380 \t Loss: 49952.40625\n",
      "Iteration: 390 \t Loss: 42323.92578125\n",
      "Iteration: 400 \t Loss: 35888.69140625\n",
      "Iteration: 410 \t Loss: 30456.68359375\n",
      "Iteration: 420 \t Loss: 25864.36328125\n",
      "Iteration: 430 \t Loss: 21980.1171875\n",
      "Iteration: 440 \t Loss: 18691.18359375\n",
      "Iteration: 450 \t Loss: 15904.0439453125\n",
      "Iteration: 460 \t Loss: 13540.0634765625\n",
      "Iteration: 470 \t Loss: 11533.923828125\n",
      "Iteration: 480 \t Loss: 9830.2861328125\n",
      "Iteration: 490 \t Loss: 8382.96484375\n",
      "Iteration: 500 \t Loss: 7152.92138671875\n",
      "Iteration: 510 \t Loss: 6106.06640625\n",
      "Iteration: 520 \t Loss: 5215.68798828125\n",
      "Iteration: 530 \t Loss: 4457.91552734375\n",
      "Iteration: 540 \t Loss: 3811.682861328125\n",
      "Iteration: 550 \t Loss: 3260.395751953125\n",
      "Iteration: 560 \t Loss: 2789.698486328125\n",
      "Iteration: 570 \t Loss: 2387.869140625\n",
      "Iteration: 580 \t Loss: 2044.561279296875\n",
      "Iteration: 590 \t Loss: 1751.2491455078125\n",
      "Iteration: 600 \t Loss: 1500.329833984375\n",
      "Iteration: 610 \t Loss: 1285.8646240234375\n",
      "Iteration: 620 \t Loss: 1102.3660888671875\n",
      "Iteration: 630 \t Loss: 945.3016967773438\n",
      "Iteration: 640 \t Loss: 810.883544921875\n",
      "Iteration: 650 \t Loss: 695.7953491210938\n",
      "Iteration: 660 \t Loss: 597.2766723632812\n",
      "Iteration: 670 \t Loss: 512.9151611328125\n",
      "Iteration: 680 \t Loss: 440.7001647949219\n",
      "Iteration: 690 \t Loss: 378.7954406738281\n",
      "Iteration: 700 \t Loss: 325.7718811035156\n",
      "Iteration: 710 \t Loss: 280.3231201171875\n",
      "Iteration: 720 \t Loss: 241.39239501953125\n",
      "Iteration: 730 \t Loss: 208.01461791992188\n",
      "Iteration: 740 \t Loss: 179.45188903808594\n",
      "Iteration: 750 \t Loss: 154.96685791015625\n",
      "Iteration: 760 \t Loss: 133.9500732421875\n",
      "Iteration: 770 \t Loss: 115.92960357666016\n",
      "Iteration: 780 \t Loss: 100.51768493652344\n",
      "Iteration: 790 \t Loss: 87.28829956054688\n",
      "Iteration: 800 \t Loss: 75.9418716430664\n",
      "Iteration: 810 \t Loss: 66.17211151123047\n",
      "Iteration: 820 \t Loss: 57.7772331237793\n",
      "Iteration: 830 \t Loss: 50.5528564453125\n",
      "Iteration: 840 \t Loss: 44.33573532104492\n",
      "Iteration: 850 \t Loss: 38.95933151245117\n",
      "Iteration: 860 \t Loss: 34.31318283081055\n",
      "Iteration: 870 \t Loss: 30.293418884277344\n",
      "Iteration: 880 \t Loss: 26.797019958496094\n",
      "Iteration: 890 \t Loss: 23.75653648376465\n",
      "Iteration: 900 \t Loss: 21.10928726196289\n",
      "Iteration: 910 \t Loss: 18.79729461669922\n",
      "Iteration: 920 \t Loss: 16.77680015563965\n",
      "Iteration: 930 \t Loss: 15.011518478393555\n",
      "Iteration: 940 \t Loss: 13.458654403686523\n",
      "Iteration: 950 \t Loss: 12.104799270629883\n",
      "Iteration: 960 \t Loss: 10.904558181762695\n",
      "Iteration: 970 \t Loss: 9.848716735839844\n",
      "Iteration: 980 \t Loss: 8.920411109924316\n",
      "Iteration: 990 \t Loss: 8.086756706237793\n",
      "Iteration: 1000 \t Loss: 7.352163314819336\n",
      "Iteration: 1010 \t Loss: 6.693521022796631\n",
      "Iteration: 1020 \t Loss: 6.115335941314697\n",
      "Iteration: 1030 \t Loss: 5.591250896453857\n",
      "Iteration: 1040 \t Loss: 5.128939151763916\n",
      "Iteration: 1050 \t Loss: 4.70980167388916\n",
      "Iteration: 1060 \t Loss: 4.334266185760498\n",
      "Iteration: 1070 \t Loss: 3.9967007637023926\n",
      "Iteration: 1080 \t Loss: 3.688725233078003\n",
      "Iteration: 1090 \t Loss: 3.411299467086792\n",
      "Iteration: 1100 \t Loss: 3.1603150367736816\n",
      "Iteration: 1110 \t Loss: 2.9306652545928955\n",
      "Iteration: 1120 \t Loss: 2.7257723808288574\n",
      "Iteration: 1130 \t Loss: 2.537396192550659\n",
      "Iteration: 1140 \t Loss: 2.366569995880127\n",
      "Iteration: 1150 \t Loss: 2.209676504135132\n",
      "Iteration: 1160 \t Loss: 2.0651988983154297\n",
      "Iteration: 1170 \t Loss: 1.9349842071533203\n",
      "Iteration: 1180 \t Loss: 1.8152852058410645\n",
      "Iteration: 1190 \t Loss: 1.705810308456421\n",
      "Iteration: 1200 \t Loss: 1.603211760520935\n",
      "Iteration: 1210 \t Loss: 1.5092072486877441\n",
      "Iteration: 1220 \t Loss: 1.424824595451355\n",
      "Iteration: 1230 \t Loss: 1.3446133136749268\n",
      "Iteration: 1240 \t Loss: 1.2713466882705688\n",
      "Iteration: 1250 \t Loss: 1.2022086381912231\n",
      "Iteration: 1260 \t Loss: 1.1391549110412598\n",
      "Iteration: 1270 \t Loss: 1.0796923637390137\n",
      "Iteration: 1280 \t Loss: 1.0242571830749512\n",
      "Iteration: 1290 \t Loss: 0.9738096594810486\n",
      "Iteration: 1300 \t Loss: 0.924949049949646\n",
      "Iteration: 1310 \t Loss: 0.8808053135871887\n",
      "Iteration: 1320 \t Loss: 0.8394163846969604\n",
      "Iteration: 1330 \t Loss: 0.7997494339942932\n",
      "Iteration: 1340 \t Loss: 0.7642089128494263\n",
      "Iteration: 1350 \t Loss: 0.7301748991012573\n",
      "Iteration: 1360 \t Loss: 0.6988257169723511\n",
      "Iteration: 1370 \t Loss: 0.6687977313995361\n",
      "Iteration: 1380 \t Loss: 0.640058696269989\n",
      "Iteration: 1390 \t Loss: 0.6139800548553467\n",
      "Iteration: 1400 \t Loss: 0.5890911221504211\n",
      "Iteration: 1410 \t Loss: 0.5654723048210144\n",
      "Iteration: 1420 \t Loss: 0.5428866147994995\n",
      "Iteration: 1430 \t Loss: 0.5219765305519104\n",
      "Iteration: 1440 \t Loss: 0.5017347931861877\n",
      "Iteration: 1450 \t Loss: 0.4828220009803772\n",
      "Iteration: 1460 \t Loss: 0.46542513370513916\n",
      "Iteration: 1470 \t Loss: 0.44868603348731995\n",
      "Iteration: 1480 \t Loss: 0.4322589635848999\n",
      "Iteration: 1490 \t Loss: 0.41746199131011963\n",
      "Iteration: 1500 \t Loss: 0.40337181091308594\n",
      "Iteration: 1510 \t Loss: 0.3892585337162018\n",
      "Iteration: 1520 \t Loss: 0.3762422502040863\n",
      "Iteration: 1530 \t Loss: 0.36367443203926086\n",
      "Iteration: 1540 \t Loss: 0.35259753465652466\n",
      "Iteration: 1550 \t Loss: 0.34129002690315247\n",
      "Iteration: 1560 \t Loss: 0.3301159739494324\n",
      "Iteration: 1570 \t Loss: 0.32003098726272583\n",
      "Iteration: 1580 \t Loss: 0.3100281059741974\n",
      "Iteration: 1590 \t Loss: 0.3012247085571289\n",
      "Iteration: 1600 \t Loss: 0.292137086391449\n",
      "Iteration: 1610 \t Loss: 0.2841412127017975\n",
      "Iteration: 1620 \t Loss: 0.27595067024230957\n",
      "Iteration: 1630 \t Loss: 0.26808950304985046\n",
      "Iteration: 1640 \t Loss: 0.26050087809562683\n",
      "Iteration: 1650 \t Loss: 0.2532583475112915\n",
      "Iteration: 1660 \t Loss: 0.24642528593540192\n",
      "Iteration: 1670 \t Loss: 0.23984722793102264\n",
      "Iteration: 1680 \t Loss: 0.23335976898670197\n",
      "Iteration: 1690 \t Loss: 0.22741742432117462\n",
      "Iteration: 1700 \t Loss: 0.22166936099529266\n",
      "Iteration: 1710 \t Loss: 0.21567830443382263\n",
      "Iteration: 1720 \t Loss: 0.21049529314041138\n",
      "Iteration: 1730 \t Loss: 0.20538553595542908\n",
      "Iteration: 1740 \t Loss: 0.20034527778625488\n",
      "Iteration: 1750 \t Loss: 0.1956474930047989\n",
      "Iteration: 1760 \t Loss: 0.19133532047271729\n",
      "Iteration: 1770 \t Loss: 0.18705913424491882\n",
      "Iteration: 1780 \t Loss: 0.1824738085269928\n",
      "Iteration: 1790 \t Loss: 0.1786986142396927\n",
      "Iteration: 1800 \t Loss: 0.174419566988945\n",
      "Iteration: 1810 \t Loss: 0.17044711112976074\n",
      "Iteration: 1820 \t Loss: 0.16693386435508728\n",
      "Iteration: 1830 \t Loss: 0.16365735232830048\n",
      "Iteration: 1840 \t Loss: 0.15985645353794098\n",
      "Iteration: 1850 \t Loss: 0.15656797587871552\n",
      "Iteration: 1860 \t Loss: 0.153346985578537\n",
      "Iteration: 1870 \t Loss: 0.15045329928398132\n",
      "Iteration: 1880 \t Loss: 0.14751678705215454\n",
      "Iteration: 1890 \t Loss: 0.14462411403656006\n",
      "Iteration: 1900 \t Loss: 0.1416713148355484\n",
      "Iteration: 1910 \t Loss: 0.1388837993144989\n",
      "Iteration: 1920 \t Loss: 0.13617713749408722\n",
      "Iteration: 1930 \t Loss: 0.13356031477451324\n",
      "Iteration: 1940 \t Loss: 0.13123789429664612\n",
      "Iteration: 1950 \t Loss: 0.12890034914016724\n",
      "Iteration: 1960 \t Loss: 0.12645041942596436\n",
      "Iteration: 1970 \t Loss: 0.1241660863161087\n",
      "Iteration: 1980 \t Loss: 0.12207333743572235\n",
      "Iteration: 1990 \t Loss: 0.1196051612496376\n",
      "Iteration: 2000 \t Loss: 0.11783145368099213\n",
      "Iteration: 2010 \t Loss: 0.11561660468578339\n",
      "Iteration: 2020 \t Loss: 0.11378873884677887\n",
      "Iteration: 2030 \t Loss: 0.11187129467725754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2040 \t Loss: 0.11002378165721893\n",
      "Iteration: 2050 \t Loss: 0.10816480964422226\n",
      "Iteration: 2060 \t Loss: 0.10641994327306747\n",
      "Iteration: 2070 \t Loss: 0.10488913953304291\n",
      "Iteration: 2080 \t Loss: 0.10316520929336548\n",
      "Iteration: 2090 \t Loss: 0.10141964256763458\n",
      "Iteration: 2100 \t Loss: 0.09998802095651627\n",
      "Iteration: 2110 \t Loss: 0.09856297075748444\n",
      "Iteration: 2120 \t Loss: 0.09698445349931717\n",
      "Iteration: 2130 \t Loss: 0.09544521570205688\n",
      "Iteration: 2140 \t Loss: 0.09415281563997269\n",
      "Iteration: 2150 \t Loss: 0.09284377098083496\n",
      "Iteration: 2160 \t Loss: 0.09155313670635223\n",
      "Iteration: 2170 \t Loss: 0.09025850892066956\n",
      "Iteration: 2180 \t Loss: 0.08908335119485855\n",
      "Iteration: 2190 \t Loss: 0.0876387283205986\n",
      "Iteration: 2200 \t Loss: 0.08646199107170105\n",
      "Iteration: 2210 \t Loss: 0.08533721417188644\n",
      "Iteration: 2220 \t Loss: 0.0843132808804512\n",
      "Iteration: 2230 \t Loss: 0.08321181684732437\n",
      "Iteration: 2240 \t Loss: 0.08200855553150177\n",
      "Iteration: 2250 \t Loss: 0.08102153986692429\n",
      "Iteration: 2260 \t Loss: 0.0796886682510376\n",
      "Iteration: 2270 \t Loss: 0.07858116924762726\n",
      "Iteration: 2280 \t Loss: 0.07765672355890274\n",
      "Iteration: 2290 \t Loss: 0.07669298350811005\n",
      "Iteration: 2300 \t Loss: 0.07568806409835815\n",
      "Iteration: 2310 \t Loss: 0.07500481605529785\n",
      "Iteration: 2320 \t Loss: 0.0740676298737526\n",
      "Iteration: 2330 \t Loss: 0.0732002705335617\n",
      "Iteration: 2340 \t Loss: 0.07233095169067383\n",
      "Iteration: 2350 \t Loss: 0.07157167047262192\n",
      "Iteration: 2360 \t Loss: 0.07075846195220947\n",
      "Iteration: 2370 \t Loss: 0.0697067454457283\n",
      "Iteration: 2380 \t Loss: 0.06902521103620529\n",
      "Iteration: 2390 \t Loss: 0.06824259459972382\n",
      "Iteration: 2400 \t Loss: 0.06758294254541397\n",
      "Iteration: 2410 \t Loss: 0.06672802567481995\n",
      "Iteration: 2420 \t Loss: 0.06608148664236069\n",
      "Iteration: 2430 \t Loss: 0.06515997648239136\n",
      "Iteration: 2440 \t Loss: 0.06436137109994888\n",
      "Iteration: 2450 \t Loss: 0.06369150429964066\n",
      "Iteration: 2460 \t Loss: 0.06307358294725418\n",
      "Iteration: 2470 \t Loss: 0.06254425644874573\n",
      "Iteration: 2480 \t Loss: 0.06154549866914749\n",
      "Iteration: 2490 \t Loss: 0.060887351632118225\n"
     ]
    }
   ],
   "source": [
    "N,D_in,H_1,H_2,D_out = 64,1000,500,100,10  \n",
    "x = torch.randn(N,D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "w1 = torch.randn(D_in,H_1,requires_grad=True) \n",
    "w2 = torch.randn(H_1,H_2,requires_grad=True)\n",
    "w3 = torch.randn(H_2,D_out,requires_grad=True)\n",
    "learning_rate = 1e-9\n",
    "for i in range (2500):\n",
    "    # forward pass \n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2).clamp(min=0).mm(w3)\n",
    "\n",
    "    # compute loss \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if i%10==0:\n",
    "        print('Iteration:',i,'\\t Loss:',loss.item())\n",
    "    # BP - compute the gradient \n",
    "    loss.backward()\n",
    "       \n",
    "    # Update weights of w1 / w2 /w3 \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad \n",
    "        w3 -= learning_rate * w3.grad \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        w3.grad.zero_()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3 Construct network by nn library in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Loss: 692.3379516601562\n",
      "Iteration: 10 \t Loss: 588.77734375\n",
      "Iteration: 20 \t Loss: 502.9967041015625\n",
      "Iteration: 30 \t Loss: 412.39837646484375\n",
      "Iteration: 40 \t Loss: 313.7581787109375\n",
      "Iteration: 50 \t Loss: 215.39002990722656\n",
      "Iteration: 60 \t Loss: 134.46014404296875\n",
      "Iteration: 70 \t Loss: 79.04597473144531\n",
      "Iteration: 80 \t Loss: 44.996551513671875\n",
      "Iteration: 90 \t Loss: 25.220855712890625\n",
      "Iteration: 100 \t Loss: 14.018439292907715\n",
      "Iteration: 110 \t Loss: 7.793113708496094\n",
      "Iteration: 120 \t Loss: 4.387673377990723\n",
      "Iteration: 130 \t Loss: 2.5204577445983887\n",
      "Iteration: 140 \t Loss: 1.4744950532913208\n",
      "Iteration: 150 \t Loss: 0.8789076805114746\n",
      "Iteration: 160 \t Loss: 0.5332027673721313\n",
      "Iteration: 170 \t Loss: 0.3286270797252655\n",
      "Iteration: 180 \t Loss: 0.20527346432209015\n",
      "Iteration: 190 \t Loss: 0.12965047359466553\n",
      "Iteration: 200 \t Loss: 0.0827036127448082\n",
      "Iteration: 210 \t Loss: 0.053206391632556915\n",
      "Iteration: 220 \t Loss: 0.03448944166302681\n",
      "Iteration: 230 \t Loss: 0.022535353899002075\n",
      "Iteration: 240 \t Loss: 0.014830032363533974\n",
      "Iteration: 250 \t Loss: 0.009814229793846607\n",
      "Iteration: 260 \t Loss: 0.006525004282593727\n",
      "Iteration: 270 \t Loss: 0.004360809922218323\n",
      "Iteration: 280 \t Loss: 0.002926220651715994\n",
      "Iteration: 290 \t Loss: 0.0019717016257345676\n",
      "Iteration: 300 \t Loss: 0.0013335890835151076\n",
      "Iteration: 310 \t Loss: 0.0009052497334778309\n",
      "Iteration: 320 \t Loss: 0.0006159311160445213\n",
      "Iteration: 330 \t Loss: 0.0004204565193504095\n",
      "Iteration: 340 \t Loss: 0.00028774552629329264\n",
      "Iteration: 350 \t Loss: 0.00019748353224713355\n",
      "Iteration: 360 \t Loss: 0.00013581366511061788\n",
      "Iteration: 370 \t Loss: 9.35939751798287e-05\n",
      "Iteration: 380 \t Loss: 6.463281897595152e-05\n",
      "Iteration: 390 \t Loss: 4.470799831324257e-05\n",
      "Iteration: 400 \t Loss: 3.097231819992885e-05\n",
      "Iteration: 410 \t Loss: 2.1507052224478684e-05\n",
      "Iteration: 420 \t Loss: 1.4948576790629886e-05\n",
      "Iteration: 430 \t Loss: 1.0404846761957742e-05\n",
      "Iteration: 440 \t Loss: 7.255805940076243e-06\n",
      "Iteration: 450 \t Loss: 5.064656306785764e-06\n",
      "Iteration: 460 \t Loss: 3.5398113595874747e-06\n",
      "Iteration: 470 \t Loss: 2.4778134957159637e-06\n",
      "Iteration: 480 \t Loss: 1.7350328107568203e-06\n",
      "Iteration: 490 \t Loss: 1.2175412393844454e-06\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "N,D_in,H_1,H_2,D_out = 64,1000,500,100,10  \n",
    "x = torch.randn(N,D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in,H_1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H_1,H_2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H_2,D_out)\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "for i in range (500):\n",
    "    # forward pass \n",
    "    y_pred = model(x)\n",
    "    # compute loss \n",
    "    loss = loss_fn(y_pred,y)\n",
    "    if i%10==0:\n",
    "        print('Iteration:',i,'\\t Loss:',loss.item())\n",
    "    # BP - compute the gradient \n",
    "    loss.backward()\n",
    "       \n",
    "    # Update weights of w1 / w2 /w3 \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4 Use optim in Pytorch to update parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Loss: 632.53271484375\n",
      "Iteration: 10 \t Loss: 499.3006896972656\n",
      "Iteration: 20 \t Loss: 394.3890075683594\n",
      "Iteration: 30 \t Loss: 295.7408447265625\n",
      "Iteration: 40 \t Loss: 205.00509643554688\n",
      "Iteration: 50 \t Loss: 127.64582824707031\n",
      "Iteration: 60 \t Loss: 69.87611389160156\n",
      "Iteration: 70 \t Loss: 32.73914337158203\n",
      "Iteration: 80 \t Loss: 12.193681716918945\n",
      "Iteration: 90 \t Loss: 3.4211552143096924\n",
      "Iteration: 100 \t Loss: 0.8538300395011902\n",
      "Iteration: 110 \t Loss: 0.25927242636680603\n",
      "Iteration: 120 \t Loss: 0.0885283574461937\n",
      "Iteration: 130 \t Loss: 0.03209293633699417\n",
      "Iteration: 140 \t Loss: 0.011802611872553825\n",
      "Iteration: 150 \t Loss: 0.004029812291264534\n",
      "Iteration: 160 \t Loss: 0.0015044229803606868\n",
      "Iteration: 170 \t Loss: 0.0005598125280812383\n",
      "Iteration: 180 \t Loss: 0.00017980953271035105\n",
      "Iteration: 190 \t Loss: 5.820178921567276e-05\n",
      "Iteration: 200 \t Loss: 2.1692698283004574e-05\n",
      "Iteration: 210 \t Loss: 8.538403562852181e-06\n",
      "Iteration: 220 \t Loss: 2.8046672468917677e-06\n",
      "Iteration: 230 \t Loss: 9.730065357871354e-07\n",
      "Iteration: 240 \t Loss: 3.379807367309695e-07\n",
      "Iteration: 250 \t Loss: 1.1710628200489737e-07\n",
      "Iteration: 260 \t Loss: 4.2945735856392275e-08\n",
      "Iteration: 270 \t Loss: 1.581487119040048e-08\n",
      "Iteration: 280 \t Loss: 5.436452621410126e-09\n",
      "Iteration: 290 \t Loss: 1.6231601618343916e-09\n",
      "Iteration: 300 \t Loss: 6.038415323317281e-10\n",
      "Iteration: 310 \t Loss: 2.697178591581917e-10\n",
      "Iteration: 320 \t Loss: 1.0648697895687675e-10\n",
      "Iteration: 330 \t Loss: 4.768986594516633e-11\n",
      "Iteration: 340 \t Loss: 2.8555604061897277e-11\n",
      "Iteration: 350 \t Loss: 1.6618257117628765e-11\n",
      "Iteration: 360 \t Loss: 1.365778497242065e-11\n",
      "Iteration: 370 \t Loss: 1.1603636454471378e-11\n",
      "Iteration: 380 \t Loss: 8.186113245600701e-12\n",
      "Iteration: 390 \t Loss: 8.342862858889966e-12\n",
      "Iteration: 400 \t Loss: 7.1504833304425475e-12\n",
      "Iteration: 410 \t Loss: 7.232320645145229e-12\n",
      "Iteration: 420 \t Loss: 6.294307089427242e-12\n",
      "Iteration: 430 \t Loss: 6.396732102342817e-12\n",
      "Iteration: 440 \t Loss: 5.583757414773238e-12\n",
      "Iteration: 450 \t Loss: 5.545586559407845e-12\n",
      "Iteration: 460 \t Loss: 6.024724122366543e-12\n",
      "Iteration: 470 \t Loss: 5.484323065130248e-12\n",
      "Iteration: 480 \t Loss: 5.14576749266471e-12\n",
      "Iteration: 490 \t Loss: 5.113911030751872e-12\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "N,D_in,H_1,H_2,D_out = 64,1000,500,100,10  \n",
    "x = torch.randn(N,D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in,H_1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H_1,H_2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H_2,D_out)\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "for i in range (500):\n",
    "    # forward pass \n",
    "    y_pred = model(x)\n",
    "    # compute loss \n",
    "    loss = loss_fn(y_pred,y)\n",
    "    if i%10==0:\n",
    "        print('Iteration:',i,'\\t Loss:',loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    # BP - compute the gradient \n",
    "    loss.backward()\n",
    "       \n",
    "    # Update weights of w1 / w2 /w3 \n",
    "    optimizer.step() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5 Build the network by class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Loss: 609.0307006835938\n",
      "Iteration: 10 \t Loss: 474.25341796875\n",
      "Iteration: 20 \t Loss: 371.3742980957031\n",
      "Iteration: 30 \t Loss: 275.36602783203125\n",
      "Iteration: 40 \t Loss: 186.88809204101562\n",
      "Iteration: 50 \t Loss: 111.91122436523438\n",
      "Iteration: 60 \t Loss: 56.8121223449707\n",
      "Iteration: 70 \t Loss: 22.95233726501465\n",
      "Iteration: 80 \t Loss: 6.67645263671875\n",
      "Iteration: 90 \t Loss: 1.4142563343048096\n",
      "Iteration: 100 \t Loss: 0.40020832419395447\n",
      "Iteration: 110 \t Loss: 0.1619892418384552\n",
      "Iteration: 120 \t Loss: 0.06336428225040436\n",
      "Iteration: 130 \t Loss: 0.020791713148355484\n",
      "Iteration: 140 \t Loss: 0.006888319738209248\n",
      "Iteration: 150 \t Loss: 0.0026579415425658226\n",
      "Iteration: 160 \t Loss: 0.0009715042542666197\n",
      "Iteration: 170 \t Loss: 0.0003233180905226618\n",
      "Iteration: 180 \t Loss: 0.00010390987881692126\n",
      "Iteration: 190 \t Loss: 3.910251325578429e-05\n",
      "Iteration: 200 \t Loss: 1.5812913261470385e-05\n",
      "Iteration: 210 \t Loss: 5.004636477679014e-06\n",
      "Iteration: 220 \t Loss: 1.50605183080188e-06\n",
      "Iteration: 230 \t Loss: 6.590852308363537e-07\n",
      "Iteration: 240 \t Loss: 2.2965673451835755e-07\n",
      "Iteration: 250 \t Loss: 7.051179551353925e-08\n",
      "Iteration: 260 \t Loss: 2.8202981283698136e-08\n",
      "Iteration: 270 \t Loss: 9.153915847548433e-09\n",
      "Iteration: 280 \t Loss: 3.1836884240021845e-09\n",
      "Iteration: 290 \t Loss: 1.2759640011239526e-09\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "N,D_in,H_1,H_2,D_out = 64,1000,500,100,10  \n",
    "x = torch.randn(N,D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "class ThreeLayerNet(torch.nn.Module):\n",
    "    def __init__(self,D_in,H_1,H_2,D_out):\n",
    "        super(ThreeLayerNet,self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in,H_1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(H_1,H_2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(H_2,D_out)\n",
    "    def forward(self,x):\n",
    "        y_pred  = self.linear3(self.relu(self.linear2(self.relu(self.linear1(x)))))\n",
    "        return y_pred \n",
    "model = ThreeLayerNet(D_in,H_1,H_2,D_out)\n",
    "# model = model.cuda()\n",
    "# device \n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "for i in range (300):\n",
    "    # forward pass \n",
    "    y_pred = model(x)\n",
    "    # compute loss \n",
    "    loss = loss_fn(y_pred,y)\n",
    "    if i%10==0:\n",
    "        print('Iteration:',i,'\\t Loss:',loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    # BP - compute the gradient \n",
    "    loss.backward()\n",
    "       \n",
    "    # Update weights of w1 / w2 /w3 \n",
    "    optimizer.step() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PY37]",
   "language": "python",
   "name": "conda-env-PY37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
