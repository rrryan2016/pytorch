{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 Image Classificaton  \n",
    "Environment: Python 3.7 & Pytorch 1.5.0+cpu on Windows 10 \n",
    "\n",
    "Keypoint:\n",
    "1. Self-defined dataset\n",
    "2. Funetune classic pre-trained models\n",
    "3. Models include AlexNet / MobileNet / MnasNet / ResNet / SqueezeNet / ShuffleNet\n",
    "4. Datasets include hymenoptera_data / MNIST / Fashion_MNIST\n",
    "\n",
    "## Content 1: Handwritting number recognition \n",
    "\n",
    "1. Use the MNIST provided by torchvision\n",
    "2. Build the network by oneself \n",
    "\n",
    "### 1.1 Preparation: Load essential packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torchvision import datasets, transforms \n",
    "import torch.utils.data as tud \n",
    "import numpy as np \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data: Load image dataset \n",
    "Load pre-defined dataset or your customized dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_1 = transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# transform_2 = transforms.Compose([\n",
    "#                                      transforms.RandomResizedCrop(input_size),\n",
    "#                                      transforms.RandomHorizontalFlip(),\n",
    "#                                      transforms.ToTensor(),\n",
    "#                                      transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "#                                  ])\n",
    "\n",
    "transform_3 = transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomGrayscale(),\n",
    "                    transforms.ToTensor(),\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST(root='./data',train=True,download=True,transform=transform_3)\n",
    "\n",
    "train_dataloader = tud.DataLoader(trainset, batch_size=100,shuffle=True,num_workers=0)\n",
    "\n",
    "testset = datasets.MNIST(root='./data',train=False,download=True,transform=transform_1)\n",
    "\n",
    "test_dataloader = tud.DataLoader(testset,batch_size=100,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Network configuration: AlexNet\n",
    "Define network / Loss function / optimization algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network in class \n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet,self).__init__()\n",
    "\n",
    "        # The pic size in MNIST is 28x28, the input pic size of AlexNet is 227x227. So network depth and parameters need to be modified. \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) #AlexCONV1(3,96, k=11,s=4,p=0)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)#AlexPool1(k=3, s=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # self.conv2 = nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)#AlexCONV2(96, 256,k=5,s=1,p=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)#AlexPool2(k=3,s=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)#AlexCONV3(256,384,k=3,s=1,p=1)\n",
    "        # self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)#AlexCONV4(384, 384, k=3,s=1,p=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)#AlexCONV5(384, 256, k=3, s=1,p=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)#AlexPool3(k=3,s=2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc6 = nn.Linear(256*3*3, 1024)  #AlexFC6(256*6*6, 4096)\n",
    "        self.fc7 = nn.Linear(1024, 512) #AlexFC6(4096,4096)\n",
    "        self.fc8 = nn.Linear(512, 10)  #AlexFC6(4096,1000)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = x.view(-1, 256 * 3 * 3)#Alex: x = x.view(-1, 256*6*6)\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "    \n",
    "net = AlexNet()   \n",
    "    \n",
    "# Define loss function \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimization function \n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Nework Training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network training function \n",
    "def train(model,train_dataloader,loss_fn,optimizer,epoch):\n",
    "    model.train()\n",
    "    for idx, (data,label) in enumerate(train_dataloader):\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % 100 == 0 :\n",
    "            print (\"Train Epoch: {}, iteration: {}, loss: {}\".format(\n",
    "            epoch,idx,loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Network evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, iteration: 0, loss: 2.3051509857177734\n",
      "Train Epoch: 0, iteration: 100, loss: 2.2957708835601807\n",
      "Train Epoch: 0, iteration: 200, loss: 2.285892963409424\n",
      "Train Epoch: 0, iteration: 300, loss: 1.3233134746551514\n",
      "Train Epoch: 0, iteration: 400, loss: 0.5398569703102112\n",
      "Train Epoch: 0, iteration: 500, loss: 0.3060709238052368\n",
      "Test Loss:0.00242247455753386, Accuracy:0.9189\n",
      "\n",
      "Train Epoch: 1, iteration: 0, loss: 0.19003471732139587\n",
      "Train Epoch: 1, iteration: 100, loss: 0.18734055757522583\n",
      "Train Epoch: 1, iteration: 200, loss: 0.19276869297027588\n",
      "Train Epoch: 1, iteration: 300, loss: 0.18399423360824585\n",
      "Train Epoch: 1, iteration: 400, loss: 0.15822146832942963\n",
      "Train Epoch: 1, iteration: 500, loss: 0.15540261566638947\n",
      "Test Loss:0.0014110058546066284, Accuracy:0.954\n",
      "\n",
      "Train Epoch: 2, iteration: 0, loss: 0.1525941789150238\n",
      "Train Epoch: 2, iteration: 100, loss: 0.10462642461061478\n",
      "Train Epoch: 2, iteration: 200, loss: 0.15674740076065063\n",
      "Train Epoch: 2, iteration: 300, loss: 0.09891148656606674\n",
      "Train Epoch: 2, iteration: 400, loss: 0.2031233161687851\n",
      "Train Epoch: 2, iteration: 500, loss: 0.11171671748161316\n",
      "Test Loss:0.0008906939765438437, Accuracy:0.9715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(model,test_dataloader,loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0 \n",
    "    correct = 0 \n",
    "    with torch.no_grad():\n",
    "        for idx,(data,label) in enumerate(test_dataloader):\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output,label)\n",
    "            pred = output.argmax(dim=1)\n",
    "            total_loss += loss \n",
    "            correct += (pred==label).sum()\n",
    "        total_loss /= len(test_dataloader.dataset)\n",
    "        acc  = correct.item()/len(test_dataloader.dataset)\n",
    "        print('Test Loss:{}, Accuracy:{}\\n'.format(total_loss,acc))\n",
    "\n",
    "# main function \n",
    "num_epochs = 3 \n",
    "for epoch in range(num_epochs):\n",
    "    train(net,train_dataloader,loss_fn,optimizer,epoch)\n",
    "    test(net,test_dataloader,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Model storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_acc = 0 \n",
    "for epoch in range(num_epochs):\n",
    "    train(net,train_dataloader,loss_fn,optimizer,epoch)\n",
    "    acc = test(model,test_dataloader,loss_fn)\n",
    "    if acc > best_valid_acc:\n",
    "        best_valid_acc = acc\n",
    "        now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "        fname=\"./trained/\"+now+r\"Best_MNIST_AlexNet.pth\"\n",
    "        torch.save(net.state_dict(),fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content 1-2: Fashion MNIST with AlexNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, iteration: 0, loss: 2.3075597286224365\n",
      "Train Epoch: 0, iteration: 100, loss: 2.2923483848571777\n",
      "Train Epoch: 0, iteration: 200, loss: 2.240149736404419\n",
      "Train Epoch: 0, iteration: 300, loss: 1.28989839553833\n",
      "Train Epoch: 0, iteration: 400, loss: 0.8979262709617615\n",
      "Train Epoch: 0, iteration: 500, loss: 0.8208087086677551\n",
      "Train Epoch: 0, iteration: 600, loss: 0.8645915985107422\n",
      "Train Epoch: 0, iteration: 700, loss: 0.7522311210632324\n",
      "Train Epoch: 0, iteration: 800, loss: 0.6402029395103455\n",
      "Train Epoch: 0, iteration: 900, loss: 0.5410202741622925\n",
      "Train Epoch: 0, iteration: 1000, loss: 0.8181690573692322\n",
      "Train Epoch: 0, iteration: 1100, loss: 0.6852773427963257\n",
      "Train Epoch: 0, iteration: 1200, loss: 0.6652778387069702\n",
      "Train Epoch: 0, iteration: 1300, loss: 0.6709299683570862\n",
      "Train Epoch: 0, iteration: 1400, loss: 0.5586152672767639\n",
      "Train Epoch: 0, iteration: 1500, loss: 1.1550471782684326\n",
      "Train Epoch: 0, iteration: 1600, loss: 0.5558242797851562\n",
      "Train Epoch: 0, iteration: 1700, loss: 0.6692951321601868\n",
      "Train Epoch: 0, iteration: 1800, loss: 0.3292231857776642\n",
      "Test Loss:0.01586221344769001, Accuracy:0.8155\n",
      "\n",
      "Train Epoch: 1, iteration: 0, loss: 0.42080143094062805\n",
      "Train Epoch: 1, iteration: 100, loss: 0.3268655240535736\n",
      "Train Epoch: 1, iteration: 200, loss: 0.45699557662010193\n",
      "Train Epoch: 1, iteration: 300, loss: 0.317310631275177\n",
      "Train Epoch: 1, iteration: 400, loss: 0.486941397190094\n",
      "Train Epoch: 1, iteration: 500, loss: 0.5999837517738342\n",
      "Train Epoch: 1, iteration: 600, loss: 0.5965442061424255\n",
      "Train Epoch: 1, iteration: 700, loss: 0.31896767020225525\n",
      "Train Epoch: 1, iteration: 800, loss: 0.27711033821105957\n",
      "Train Epoch: 1, iteration: 900, loss: 0.5971960425376892\n",
      "Train Epoch: 1, iteration: 1000, loss: 0.34339606761932373\n",
      "Train Epoch: 1, iteration: 1100, loss: 0.35725998878479004\n",
      "Train Epoch: 1, iteration: 1200, loss: 0.23863334953784943\n",
      "Train Epoch: 1, iteration: 1300, loss: 0.4400840103626251\n",
      "Train Epoch: 1, iteration: 1400, loss: 0.3215658664703369\n",
      "Train Epoch: 1, iteration: 1500, loss: 0.2469744086265564\n",
      "Train Epoch: 1, iteration: 1600, loss: 0.5537420511245728\n",
      "Train Epoch: 1, iteration: 1700, loss: 0.4772762060165405\n",
      "Train Epoch: 1, iteration: 1800, loss: 0.5159320831298828\n",
      "Test Loss:0.011939968913793564, Accuracy:0.8596\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-55a41d10b890>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"./trained/fashion_mnist_cnn.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Fashion_MNIST need to be downloaded in preparation\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "train_dataloader = tud.DataLoader(\n",
    "    datasets.FashionMNIST(\"./datasets/fashion_mnist_data\",train=True,download=True,\n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize(mean=(0.2860402,),std=(0.3530239,))\n",
    "                         ])),batch_size = batch_size,shuffle = True)\n",
    "\n",
    "test_dataloader = tud.DataLoader(\n",
    "    datasets.FashionMNIST(\"./datasets/fashion_mnist_data\",train=False,download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize(mean=(0.2860402,),std=(0.3530239,))\n",
    "                   ])),batch_size=batch_size) \n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "net = AlexNet()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr=lr,momentum=momentum)\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train(net,train_dataloader,loss_fn,optimizer,epoch)\n",
    "    test(net,test_dataloader,loss_fn)\n",
    "    \n",
    "torch.save(net.state_dict(),\"./trained/fashion_mnist_cnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content 2: Classification of ant and bee \n",
    "1. Customized dataset:hymenoptera_data\n",
    "2. Finetune based on existing model(ResNet-18/MobileNet/ShuffleNet/...)\n",
    "\n",
    "### 1. Package Perparation \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.utils.data as tud\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data perparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The download link of hymenoptera_data: https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "\n",
    "data_dir = './datasets/hymenoptera_data'\n",
    "\n",
    "num_class =2 \n",
    "input_size = 224 \n",
    "batch_size = 32 \n",
    "\n",
    "# all_imgs = datasets.ImageFolder(os.path.join(data_dir,\"train\"),\n",
    "#                                transform = transforms.Compose([\n",
    "#                                    transforms.RandomResizedCrop(input_size),\n",
    "#                                    transforms.RandomHorizontalFlip(),\n",
    "#                                    transforms.ToTensor()\n",
    "#                                ]))\n",
    "# loader = tud.DataLoader(all_imgs,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "train_imgs = datasets.ImageFolder(os.path.join(data_dir,\"train\"),\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(input_size),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])\n",
    "                                ]))\n",
    "train_dataloader = tud.DataLoader(train_imgs,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "test_imgs = datasets.ImageFolder(os.path.join(data_dir,\"val\"),\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.Resize(input_size),  \n",
    "                                    transforms.CenterCrop(input_size),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])\n",
    "                                ]))\n",
    "test_dataloader = tud.DataLoader(test_imgs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Network configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.2, inplace=True)\n",
      "  (1): Linear(in_features=1280, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define network \n",
    "def initialize_model (model_name,num_class,use_pretrained=True,feature_extract=True):\n",
    "    if model_name == 'resnet18': \n",
    "        # Option1:Download the pth file \n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)        \n",
    "    `   # Option2: Use the pth file stored in local computer \n",
    "#         model_ft = models.resnet18(pretrained=False)\n",
    "#         model_ft.load_state_dict(torch.load('./trained/resnet18-5c106cde.pth'))        \n",
    "        if feature_extract:\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False \n",
    "        num_ftrs = model_ft.fc.in_features \n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_class)\n",
    "        \n",
    "    elif model_name == 'squeezenet1_0': \n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)             \n",
    "        if feature_extract:\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False \n",
    "        model_ft.classifier[1] = nn.Conv2d(512,num_class,kernel_size=(1,1),stride=(1,1))\n",
    "        model_ft.num_classes=num_class   \n",
    "        \n",
    "    elif model_name == 'mnasnet0_5':\n",
    "        model_ft = models.mnasnet0_5(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False         \n",
    "        model_ft.classifier[1].out_features = num_class\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    elif model_name == 'mobilenet_v2': \n",
    "        model_ft = models.mobilenet_v2(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False \n",
    "        model_ft.classifier[1].out_features = num_class\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = True\n",
    "     \n",
    "    elif model_name == 'shufflenet_v2_x0_5': \n",
    "        model_ft = models.shufflenet_v2_x0_5(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False \n",
    "        num_ftrs = model_ft.fc.in_features \n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_class)  \n",
    "        \n",
    "    else:\n",
    "        print('Model not implemented.')\n",
    "        return None \n",
    "    return model_ft\n",
    "\n",
    "# To use different pretrained model, change the 'resnet18' to different model_name in initialize_model\n",
    "model_ft = initialize_model(\"resnet18\",2,use_pretrained=True,feature_extract=True)\n",
    "\n",
    "# More pretrained models, check:https://pytorch.org/docs/master/torchvision/models.html?highlight=torchvision%20models\n",
    "# Selected Pre-trained models(resnet18/squeezenet1_0/mnasnet0_5/mobilenet_v2/shufflenet_v2_x0_5) in this code whose .pth file to easy to download \n",
    "\n",
    "# Define Loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimization function, select optimal parameter for different model checked\n",
    "\n",
    "# Parameters for resnet18 / squeezenet1_0 / mobilenet_v2\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "\n",
    "# Parameters for shufflenet_v2_x0_5\n",
    "# lr = 0.05  \n",
    "# momentum = 0.5\n",
    "\n",
    "# Parameters for mnasnet\n",
    "# lr = 0.1 \n",
    "# momentum = 0.9\n",
    "\n",
    "optimizer = optim.SGD(model_ft.parameters(),lr=lr,momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Network training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_dataloader,loss_fn,optimizer,epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    total_corrects = 0.\n",
    "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total_corrects += torch.sum(preds.eq(labels))\n",
    "    epoch_loss = total_loss / len(train_dataloader.dataset)\n",
    "    epoch_accuracy = total_corrects / len(train_dataloader.dataset)\n",
    "    print(\"Epoch:{}, Training Loss:{:.4f}, Traning Acc:{:.4f}\".format(epoch,epoch_loss,epoch_accuracy))  \n",
    "      \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model evaluation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,test_dataloader,loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_corrects = 0.\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(test_dataloader):\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_corrects += torch.sum(preds.eq(labels))\n",
    "    epoch_loss = total_loss / len(test_dataloader.dataset)\n",
    "    epoch_accuracy = total_corrects / len(test_dataloader.dataset)\n",
    "    print(\"Test Loss:{:.4f}, Test Acc:{:.4f}\".format(epoch_loss,epoch_accuracy))  \n",
    "    return epoch_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Training Loss:6.0485, Traning Acc:0.1025\n",
      "Test Loss:2.8012, Test Acc:0.1765\n",
      "Epoch:1, Training Loss:0.7612, Traning Acc:0.7459\n",
      "Test Loss:0.5049, Test Acc:0.7843\n",
      "Epoch:2, Training Loss:0.3640, Traning Acc:0.8361\n",
      "Test Loss:0.4426, Test Acc:0.8301\n",
      "Epoch:3, Training Loss:0.3488, Traning Acc:0.8566\n",
      "Test Loss:0.3897, Test Acc:0.8366\n",
      "Epoch:4, Training Loss:0.2931, Traning Acc:0.8852\n",
      "Test Loss:0.3080, Test Acc:0.8954\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "   train_model(model_ft,train_dataloader,loss_fn,optimizer,epoch)\n",
    "   test_model(model_ft,test_dataloader,loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PY37]",
   "language": "python",
   "name": "conda-env-PY37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
